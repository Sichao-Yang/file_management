上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法--决策树（Decision Tree）。

# **4、决策树**

## **4.1 决策树基本概念**

顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：

*****
      女儿：多大年纪了？
      母亲：26。
      女儿：长的帅不帅？
      母亲：挺帅的。
      女儿：收入高不？
      母亲：不算很高，中等情况。
      女儿：是公务员不？
      母亲：是，在税务局上班呢。
      女儿：那好，我去见见。
*****

这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。

![1.png](media\5bc728ec84a77.png)

在上图的决策树中，决策过程的每一次判定都是对某一属性的"测试"，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：

    * 每个非叶节点表示一个特征属性测试。
    * 每个分支代表这个特征属性在某个值域上的输出。
    * 每个叶子节点存放一个类别。
    * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。

## **4.2 决策树的构造**

决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：

![2.png](media\5bc728ecc27fe.png)

可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地"纯"，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。

### **4.2.1 ID3算法**

#### 信息熵

ID3算法使用信息增益为准则来选择划分属性，"信息熵"(information entropy)是度量样本结合纯度的常用指标，代表一个系统中蕴含多少信息量。信息量越大表明一个系统不确定性就越大，就存在越多的可能性，即信息熵越大。

假定当前样本集合D中第k类样本所占比例为p~k~（k=1,2,...,|y|），则样本集合D的信息熵定义为：
$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|y|} p_{k} \log _{2} p_{k}   \tag{4.1}
$$
eq.4.1 解析见南瓜书

#### 信息增益

定义：

假定离散特征a有V个可能的取值{a^1^, a^2^, ..., a^v^}，如果使用特征a来对数据集D进行划分，则会产生V个分支节点。其中第i个节点包含了数据集D种所有在特征a上取值为a^i^的样本总数，记为D^i^。我们首先算出该分支节点上的信息熵。再根据该分支节点所包含的样本数量D^i^取权重。

把所有的分支节点相加，再被原始集D的信息熵减掉，就能够计算出特征a对原始样本集D进行划分所获得的"信息增益"（information gain）：
$$
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)  \tag{4.2}
$$
划分出的每个分支节点上的信息纯度越好，信息增益越大，表示使用该属性划分样本集D的效果越好。因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。

这里举了一个书上的信息增益的计算例子：

![vlcsnap-2020-02-26-12h40m19s144](media\vlcsnap-2020-02-26-12h40m19s144.png)

我把特征a，色泽的计算方法放出来了，其他5个特征的计算方法类似。都算出来后，在里面选增益最大的特征做划分属性。接着一步步划分下去，就能得到下图里的树状结构。

![vlcsnap-2020-02-26-12h45m22s870](media\vlcsnap-2020-02-26-12h45m22s870.png)

ID3算法的不足：采用信息增益大的特征会导致属性划分较多的比属性划分少的特征信息增益大。

### **4.2.2 C4.5算法**

ID3算法最核心的问题是偏向于取值数目较多的属性，极端的例子是，存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类用处不大。

因此C4.5算法使用了"增益率"（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：

#### eq. 4.3 & 4.4

$$
\text { Gain }_{-} \text {ratio }(D, a)=\frac{\operatorname{Gain}(D, a)}{I V(a)}
$$

其中IV(a)是属性a的固有值：
$$
I V(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}
$$
当属性取值越多时，IV(a)的值越大。上面这个式子就不推导了。

C4.5的算法不足：

1. 每次生成的是多叉树，完成决策树生成的效率较慢。
2. 由于使用了熵的模型，有大量耗时的对数运算。
3. 如果是连续值特征的话，还要进行排序运算，更加耗时。
4. 只能处理分类问题。

### **4.2.3 CART算法**

CART决策树使用"基尼指数"（Gini index）来选择划分属性。基尼指数是基于基尼值的。

#### 基尼值

用于度量数据集的纯度，Gini(D)反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小表示数据集 的纯度越高。

假定当前样本集合D中第k类样本所占的比例为p~k~（k=1,2,...,|y|），则D的基尼值为：
$$
\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{|y|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
&=p_{1} \cdot\left(p_{2}+p_{3}+\ldots+p_{k}\right)+\ldots+p_{k} \cdot\left(p_{1}+p_{2}+p_{3}+\ldots+p_{k-1}\right) \\
&=p_{1} \cdot\left(p_{2}+p_{3}+\ldots+p_{k}\right)+\ldots+p_{k}\left(1-p_{k}\right) \\
&=\left(p_{1}+p_{2}+\ldots+p_{k}\right)-\left(p_{1}^{2}+p_{2}^{2}+\ldots+p_{k}^{2}\right) \\
&=1-\sum_{k=1}^{|y|} p_{k}^{2}
\end{aligned}
$$

#### 基尼指数

同样的，我们对划分为a^v^值的样本集的大小D^v^取权重再sum后就可以得到基尼指数：
$$
\text { Gini }_{-} \text {index }(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)
$$
可以看到==判定条件里没有了对数log，所以计算量小了很多==。

## **4.3 剪枝处理**

从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：

* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。

评估方式是对决策树的泛化能力进行度量。这里可以使用测试集作为学习器泛化性能的近似，因此我们将数据集划分为训练集和测试集。

预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。

后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。

![8.png](media\5bc728ec80d34.png)

![9.png](media\5bc728ec9e330.png)

![10.png](media\5bc728ec9d497.png)

上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝"贪心"的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。

而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其需要自底向上遍历了所有节点，对训练时间的开销相比预剪枝大大提升。

## **4.4 连续值与缺失值处理**

### 连续值处理

对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。

* 首先将a的所有取值按升序排列，记为：$\left\{a^{1}, a^{2}, \ldots, a^{n}\right\}$。所有相邻属性的均值作为候选划分点（n-1个，n为a所有的取值数目）：
  $$
  T_{a}=\left\{\frac{a^{i}+a^{i+1}}{2} | 1 \leqslant i \leqslant n-1\right\}
  $$

* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益

* 选择最大信息增益的划分点作为最优划分点：

![11.png](media\5bc72a0968fad.png)

### 缺失值处理

现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：

1. 如何进行划分属性的选择。
2. 给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。

* 对于问题一：


我们首先来定义三个公式：

![image-20200226142517944](media\image-20200226142517944.png)

第一个式子即无缺失样本所占比例。第二个式子是每个类所占比例。第三个式子是每个属性取值所占比例。

![12.png](media\5bc72a098f3be.png)

根据上式，我们可以把信息增益推广为：

把原始的公式里的样本集D用子集来代入，同时用考虑权重w的方式来改写其他参数，算出子集信息增益，最后再乘上子集所占比重。

![13.png](media\5bc72a096ccc3.png)
$$
\operatorname{Ent}(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k} \log _{2} \tilde{p}_{k}
$$

书上举了个例子：

![image-20200226144211985](media\image-20200226144211985.png)

![image-20200226143737746](media\image-20200226143737746.png)

![image-20200226143815085](media\image-20200226143815085.png)

* 对于问题2：

这其实是已经选好了划分属性后，为划分好的子节点的下一个属性划分做的准备工作：计算现节点的ENT(D)

若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中：

![image-20200226144149034](media\image-20200226144149034.png)

![image-20200226144238887](media\image-20200226144238887.png)

根据纹理的属性划分完成，我们把【稍糊】节点拿出，进行进一步划分：考虑属性【色泽】。用于现在8，10的权重都是1/3所以相加为2/3。13号属性缺失。

![image-20200226144825748](media\image-20200226144825748.png)

接着上图底部我们计算r1（乌黑）r2（青绿）r3（浅白），再算出不同分支的信息熵：

![image-20200226144952295](media\image-20200226144952295.png)

再算出属性a的信息增益（加和，加权）：

![image-20200226145024621](media\image-20200226145024621.png)

然后再对不同候选属性进行上述计算，再对比不同属性切分效果，选出增益最大的完成划分。

---------



# 南瓜书

## 4.1

$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_{2}{p_k}
$$

[解析]：证明$0\leq\operatorname{Ent}(D)\leq\log_{2}|\mathcal{Y}|$：
已知集合$D$的信息熵的定义为
$$
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}
$$
其中，$|\mathcal{Y}|$表示样本类别总数，$p_k$表示第$k$类样本所占的比例，且$0 \leq p_k \leq 1,\sum_{k=1}^{n}p_k=1$。若令$|\mathcal{Y}|=n,p_k=x_k$，那么信息熵$\operatorname{Ent}(D)$就可以看作一个$n$元实值函数，也即
$$
\operatorname{Ent}(D)=f(x_1,...,x_n)=-\sum_{k=1}^{n} x_{k} \log _{2} x_{k}
$$

其中，$0 \leq x_k \leq 1,\sum_{k=1}^{n}x_k=1$，下面考虑求该多元函数的最值。首先我们先来求最大值，如果不考虑约束$0 \leq x_k \leq 1$，仅考虑$\sum_{k=1}^{n}x_k=1$的话，对$f(x_1,...,x_n)$求最大值等价于如下最小化问题
$$
\begin{array}{ll}{
\operatorname{min}} & {\sum\limits_{k=1}^{n} x_{k} \log _{2} x_{k} } \\
{\text { s.t. }} & {\sum\limits_{k=1}^{n}x_k=1}
\end{array}
$$

显然，在$0 \leq x_k \leq 1$时，此问题为凸优化问题，而对于凸优化问题来说，能令其拉格朗日函数的一阶偏导数等于0的点即为最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为
$$
L(x_1,...,x_n,\lambda)=\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda(\sum_{k=1}^{n}x_k-1)
$$

其中，$\lambda$为拉格朗日乘子。对$L(x_1,...,x_n,\lambda)$分别关于$x_1,...,x_n,\lambda$求一阶偏导数，并令偏导数等于0可得
$$
\begin{aligned}
\cfrac{\partial L(x_1,...,x_n,\lambda)}{\partial x_1}&=\cfrac{\partial }{\partial x_1}\left[\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda(\sum_{k=1}^{n}x_k-1)\right]=0\\
&=\log _{2} x_{1}+x_1\cdot \cfrac{1}{x_1\ln2}+\lambda=0 \\
&=\log _{2} x_{1}+\cfrac{1}{\ln2}+\lambda=0 \\
&\Rightarrow \lambda=-\log _{2} x_{1}-\cfrac{1}{\ln2}\\
\cfrac{\partial L(x_1,...,x_n,\lambda)}{\partial x_2}&=\cfrac{\partial }{\partial x_2}\left[\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda(\sum_{k=1}^{n}x_k-1)\right]=0\\
&\Rightarrow \lambda=-\log _{2} x_{2}-\cfrac{1}{\ln2}\\
\vdots\\
\cfrac{\partial L(x_1,...,x_n,\lambda)}{\partial x_n}&=\cfrac{\partial }{\partial x_n}\left[\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda(\sum_{k=1}^{n}x_k-1)\right]=0\\
&\Rightarrow \lambda=-\log _{2} x_{n}-\cfrac{1}{\ln2}\\
\cfrac{\partial L(x_1,...,x_n,\lambda)}{\partial \lambda}&=\cfrac{\partial }{\partial \lambda}\left[\sum_{k=1}^{n} x_{k} \log _{2} x_{k}+\lambda(\sum_{k=1}^{n}x_k-1)\right]=0\\
&\Rightarrow \sum_{k=1}^{n}x_k=1\\
\end{aligned}
$$

整理一下可得
$$
\left\{ \begin{array}{lr}
\lambda=-\log _{2} x_{1}-\cfrac{1}{\ln2}=-\log _{2} x_{2}-\cfrac{1}{\ln2}=...=-\log _{2} x_{n}-\cfrac{1}{\ln2} \\
\sum\limits_{k=1}^{n}x_k=1
\end{array}\right.
$$

由以上两个方程可以解得
$$
x_1=x_2=...=x_n=\cfrac{1}{n}
$$
又因为$x_k$还需满足约束$0 \leq x_k \leq 1$，显然$0 \leq\cfrac{1}{n}\leq 1$，所以$x_1=x_2=...=x_n=\cfrac{1}{n}$是满足所有约束的最优解，也即为当前最小化问题的最小值点，同时也是$f(x_1,...,x_n)$的最大值点。将$x_1=x_2=...=x_n=\cfrac{1}{n}$代入$f(x_1,...,x_n)$中可得
$$
f(\cfrac{1}{n},...,\cfrac{1}{n})=-\sum_{k=1}^{n} \cfrac{1}{n} \log _{2} \cfrac{1}{n}=-n\cdot\cfrac{1}{n} \log _{2} \cfrac{1}{n}=\log _{2} n
$$
所以$f(x_1,...,x_n)$在满足约束$0 \leq x_k \leq 1,\sum_{k=1}^{n}x_k=1$时的最大值为$\log _{2} n$。求完最大值后下面我们再来求最小值，如果不考虑约束$\sum_{k=1}^{n}x_k=1$，仅考虑$0 \leq x_k \leq 1$的话，$f(x_1,...,x_n)$可以看做是$n$个互不相关的一元函数的加和，也即 $f(x_1,...,x_n)=\sum_{k=1}^{n} g(x_k) $。其中，$g(x_k)=-x_{k} \log _{2} x_{k},0 \leq x_k \leq 1$。

那么当$g(x_1),g(x_2),...,g(x_n)$分别取到其最小值时，$f(x_1,...,x_n)$也就取到了最小值。所以接下来考虑分别求$g(x_1),g(x_2),...,g(x_n)$各自的最小值，由于$g(x_1),g(x_2),...,g(x_n)$的定义域和函数表达式均相同，所以只需求出$g(x_1)$的最小值也就求出了$g(x_2),...,g(x_n)$的最小值。下面考虑求$g(x_1)$的最小值，首先对$g(x_1)$关于$x_1$求一阶和二阶导数
$$
g^{\prime}(x_1)=\cfrac{d(-x_{1} \log _{2} x_{1})}{d x_1}=-\log _{2} x_{1}-x_1\cdot \cfrac{1}{x_1\ln2}=-\log _{2} x_{1}-\cfrac{1}{\ln2}
$$

$$
g^{\prime\prime}(x_1)=\cfrac{d\left(g^{\prime}(x_1)\right)}{d x_1}=\cfrac{d\left(-\log _{2} x_{1}-\cfrac{1}{\ln2}\right)}{d x_1}=-\cfrac{1}{x_{1}\ln2}
$$

显然，当$0 \leq x_k \leq 1$时$g^{\prime\prime}(x_1)=-\cfrac{1}{x_{1}\ln2}$恒小于0，所以$g(x_1)$是一个在其定义域范围内开口向下的凹函数，那么其最小值必然在边界取，于是分别取$x_1=0$和$x_1=1$，代入$g(x_1)$可得
$$
g(0)=-0\log _{2} 0=0
$$

$$
g(1)=-1\log _{2} 1=0
$$

所以，$g(x_1)$的最小值为0，同理可得$g(x_2),...,g(x_n)$的最小值也为0，那么$f(x_1,...,x_n)$的最小值此时也为0。但是，此时是不考虑约束$\sum_{k=1}^{n}x_k=1$，仅考虑$0 \leq x_k \leq 1$时取到的最小值，若考虑约束$\sum_{k=1}^{n}x_k=1$的话，那么$f(x_1,...,x_n)$的最小值一定大于等于0。如果令某个$x_k=1$，那么根据约束$\sum_{k=1}^{n}x_k=1$可知$x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，将其代入$f(x_1,...,x_n)$可得
$$f(0,0,...,0,1,0,...,0)=-0 \log _{2}0-0 \log _{2}0...-0 \log _{2}0-1 \log _{2}1-0 \log _{2}0...-0 \log _{2}0=0 $$
所以$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$一定是$f(x_1,...,x_n)$在满足约束$\sum_{k=1}^{n}x_k=1$和$0 \leq x_k \leq 1$的条件下的最小值点，其最小值为0。

综上可知，当$f(x_1,...,x_n)$取到最大值时：$x_1=x_2=...=x_n=\cfrac{1}{n}$，此时样本集合纯度最低；当$f(x_1,...,x_n)$取到最小值时：$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，此时样本集合纯度最高。

## 4.2

$$\operatorname{Gain}(D,a) = \operatorname{Ent}(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Ent}({D^v})$$

[解析]：这个是信息增益的定义公式，在信息论中信息增益也称为互信息（参见附录①），其表示已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度。所以在这里，这个公式可以理解为在属性$a$的取值已知后数据集$D$中类别$k$的不确定性减小的程度。若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，也即为书上所说的"纯度提升"越大。

## 4.6

$$\operatorname{Gini\_index}(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}\operatorname{Gini}(D^v)$$

[解析]：这个是数据集$D$中属性$a$的基尼指数的定义，它表示在属性$a$的取值已知的条件下，数据集$D$按照属性$a$的所有可能取值划分后的纯度，不过在构造CART分类树时并不会严格按照此公式来选择最优划分属性，主要是==因为CART分类树是一颗二叉树，如果用上面的公式去选出最优划分属性，无法进一步选出最优划分属性的最优划分点==。CART分类树的构造算法如下：

- 首先，对每个属性$a$的每个可能取值$v$，将数据集$D$分为$a=v$和$a\neq v$两部分来计算基尼指数，即
  $$
  \operatorname{Gini\_index}(D,a) = \frac{|D^{a=v}|}{|D|}\operatorname{Gini}(D^{a=v})+\frac{|D^{a\neq v}|}{|D|}\operatorname{Gini}(D^{a\neq v})
  $$

- 然后，选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；

- 最后，重复以上两步，直至满足停止条件。

下面以西瓜书中表4.2中西瓜数据集2.0为例来构造CART分类树，其中第一个最优划分属性和最优划分点的计算过程如下：
以属性"色泽"为例，它有3个可能的取值：$\{\text{青绿}，\text{乌黑}，\text{浅白}\}$， 若使用该属性的属性值是否等于"青绿"对数据集$D$进行划分，则可得到2个子集，分别记为$D^1(\text{色泽}=\text{青绿}),D^2(\text{色泽}\not=\text{青绿})$。子集$D^1$包含编号$\{1,4,6,10,13,17\}$共6个样例，其中正例占$p_1=\frac{3}{6}$，反例占$p_2=\frac{3}{6}$；子集$D^2$包含编号$\{2,3,5,7,8,9,11,12,14,15,16\}$共11个样例，其中正例占$p_1=\frac{5}{11}$，反例占$p_2=\frac{6}{11}$，根据公式（4.5）可计算出用"色泽=青绿"划分之后得到基尼指数为

$$
\operatorname{Gini\_index}(D,\text{色泽}=\text{青绿}) = \frac{6}{17}\times\left(1-(\frac{3}{6})^2-(\frac{3}{6})^2\right)+\frac{11}{17}\times\left(1-(\frac{5}{11})^2-(\frac{6}{11})^2\right)
= 0.497
$$
类似的，可以计算出以下不同属性取不同值的基尼指数
$$
\begin{aligned}
\operatorname{Gini\_index}(D,\text{色泽}=\text{乌黑}) = \frac{6}{17}\times\left(1-(\frac{4}{6})^2-(\frac{2}{6})^2\right)+\frac{11}{17}\times\left(1-(\frac{4}{11})^2-(\frac{7}{11})^2\right)
= 0.456\\
\operatorname{Gini\_index}(D,\text{色泽}=\text{浅白}) = \frac{5}{17}\times\left(1-(\frac{1}{5})^2-(\frac{4}{5})^2\right)+\frac{12}{17}\times\left(1-(\frac{7}{12})^2-(\frac{5}{12})^2\right)
= 0.426
\end{aligned}
$$

$$
\begin{aligned}
\operatorname{Gini\_index}(D,\text{根蒂}=\text{蜷缩}) = 0.456\\
\operatorname{Gini\_index}(D,\text{根蒂}=\text{稍蜷}) = 0.496\\
\operatorname{Gini\_index}(D,\text{根蒂}=\text{硬挺}) = 0.439\\
\operatorname{Gini\_index}(D,\text{敲声}=\text{浊响}) = 0.450\\
\operatorname{Gini\_index}(D,\text{敲声}=\text{沉闷}) = 0.494\\
\operatorname{Gini\_index}(D,\text{敲声}=\text{清脆}) = 0.439\\
\operatorname{Gini\_index}(D,\text{纹理}=\text{清晰}) = 0.286\\
\operatorname{Gini\_index}(D,\text{纹理}=\text{稍稀}) = 0.437\\
\operatorname{Gini\_index}(D,\text{纹理}=\text{模糊}) = 0.403\\
\operatorname{Gini\_index}(D,\text{脐部}=\text{凹陷}) = 0.415\\
\operatorname{Gini\_index}(D,\text{脐部}=\text{稍凹}) = 0.497\\
\operatorname{Gini\_index}(D,\text{脐部}=\text{平坦}) = 0.362\\
\operatorname{Gini\_index}(D,\text{触感}=\text{硬挺}) = 0.494\\
\operatorname{Gini\_index}(D,\text{触感}=\text{软粘}) = 0.494
\end{aligned}
$$

特别地，对于属性"触感"，由于它的可取值个数为2，所以其实只需计算其中一个取值的基尼指数即可。根据上面的计算结果可知$\operatorname{Gini\_index}(D,\text{纹理}=\text{清晰}) = 0.286$最小，所以选择属性"纹理"为最优划分属性并生成根节点，接着以"纹理=清晰"为最优划分点生成$D^1(\text{纹理}=\text{清晰}),D^2(\text{纹理}\not=\text{清晰})$两个子节点，对于两个子节点分别重复上述步骤继续生成下一层子节点，直至满足停止条件。以上便是CART分类树的构建过程，从构建过程中可以看出，CART分类树最终构造出来的是一颗二叉树。CART决策树除了能处理分类问题以外，它还可以处理回归问题，附录②中给出了CART回归树的构造算法。

## 4.7

$$
T_a={\lbrace{\frac{a^i+a^{i+1}}{2}|1\leq{i}\leq{n-1}}\rbrace}
$$

[解析]：这个公式所表达的思想很简单，就是以每两个相邻取值的中点作为划分点，下面以西瓜书中表4.3中西瓜数据集3.0为例来说明此公式的用法。对于"密度"这个连续属性，已观测到的可能取值为$\{0.243,0.245,0.343,0.360,0.403,0.437,0.481,0.556,0.593,0.608,0.634,0.639,0.657,0.666,0.697,0.719,0.774\}$共17个值，根据公式（4.7）可知，此时$i$依次取1到16，那么"密度"这个属性的候选划分点集合为
$T_{a} = \{\frac{(0.243+0.245)}{2}, \frac{(0.245+0.343)}{2},\frac{(0.343+0.360)}{2},\frac{(0.360+0.403)}{2},\frac{(0.403+0.437)}{2},\frac{(0.437+0.481)}{2},\frac{(0.481+0.556)}{2},\\\frac{(0.556+0.593)}{2},\frac{(0.593+0.608)}{2},\frac{(0.608+0.634)}{2},\frac{(0.634+0.639)}{2},\frac{(0.639+0.657)}{2},\frac{(0.657+0.666)}{2},\frac{(0.666+0.697)}{2},\frac{(0.697+0.719)}{2},\frac{(0.719+0.774)}{2}\}$

## 4.8

$$
\begin{aligned}
\operatorname{Gain}(D,a) &= \max_{t\in{T_a}}\operatorname{Gain}(D,a,t)\\
&=
\max_{t\in{T_a}}[\operatorname{Ent}(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_t^{\lambda}|}
{|D|}\operatorname{Ent}(D_t^{\lambda})]
\end{aligned}
$$

[解析]：此公式是公式（4.2）用于离散化后的连续属性的版本，其中$T_a$由公式（4.7）计算得来，$\lambda\in\{-,+\}$表示属性$a$的取值分别小于等于和大于候选划分点$t$时的情形，也即当$\lambda=-$时：$D^{\lambda}_t=D^{a\leq t}_t$，当$\lambda=+$时：$D^{\lambda}_t=D^{a> t}_t$。


## 附录

### ①互信息<sup>[1]</sup>

在解释互信息之前，需要先解释一下什么是条件熵。条件熵表示的是在已知一个随机变量的条件下，另一个随机变量的不确定性。具体地，假设有随机变量$X$和$Y$，且它们服从以下联合概率分布
$$
P(X = x_{i},Y = y_{j}) = p_{ij}\quad i = 1,2,....,n;j = 1,2,...,m
$$
那么在已知$X$的条件下，随机变量$Y$的条件熵为
$$
\operatorname{Ent}(Y|X) =  \sum_{i=1}^np_i \operatorname{Ent}(Y|X = x_i)
$$
其中，$ p_i = P(X = x_i) ，i =1,2,...,n$。互信息定义为信息熵和条件熵的差，它表示的是已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度。具体地，假设有随机变量$X$和$Y$，那么在已知$X$的信息后，$Y$的不确定性减少的程度为
$$
\operatorname{I}(Y;X) = \operatorname{Ent}(Y) - \operatorname{Ent}(Y|X)
$$
此即为互信息的数学定义。

### ②CART回归树<sup>[1]</sup>

假设给定数据集
$$
D = {(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2)...,(\boldsymbol{x}_N,y_N)}
$$
其中$\boldsymbol{x}\in \mathbb{R}^d$为$d$维特征向量，$y\in \mathbb{R}$是连续型随机变量，这是一个标准的回归问题的数据集。若把每个属性视为坐标空间中的一个坐标轴，则$d$个属性就构成了一个$d$维的特征空间，而每个$d$维特征向量$\boldsymbol{x}$就对应了$d$维的特征空间中的一个数据点。CART回归树的目标是将特征空间划分成若干个子空间，每个子空间都有一个固定的输出值，也就是凡是落在同一个子空间内的数据点$\boldsymbol{x}_i$，他们所对应的输出值$y_i$恒相等，且都为该子空间的输出值。那么如何划分出若干个子空间呢？这里采用一种启发式的方法：

- 任意选择一个属性$a$，遍历其所有可能取值，根据如下公式找出属性$a$最优划分点$v^*$：
  $$
  v^* = \arg\min_{v}\left[\min_{c_1}\sum_{\boldsymbol {x}_i\in{R_1(a,v)}}{(y_i - c_1)}^2 + \min_{c_2}\sum_{\boldsymbol {x}_i\in{R_2(a,v)}}{(y_i - c_2)}^2 \right]
  $$
  其中，$R_1(a,v)=\{\boldsymbol {x}|\boldsymbol {x}\in D^{a\leq v}\},R_2(a,v)=\{\boldsymbol {x}|\boldsymbol {x}\in D^{a > v}\}$，$c_1$和$c_2$分别为集合$R_1(a,v)$和$R_2(a,v)$中的样本$\boldsymbol {x}_i$对应的输出值$y_i$的均值，也即
  $$
  c_1=\operatorname{ave}(y_i | x\in R_1(a,v))=\frac{1}{|R_1(a,v)|}\sum_{\boldsymbol {x}_i\in{R_1(a,v)}}y_i
  $$

  $$
  c_2=\operatorname{ave}(y_i | x\in R_2(a,v))=\frac{1}{|R_2(a,v)|}\sum_{\boldsymbol {x}_i\in{R_2(a,v)}}y_i
  $$

- 遍历所有属性，找到最优划分属性$a^*$，然后根据$a^*$的最优划分点$v^*$将特征空间划分为两个子空间，接着对每个子空间重复上述步骤，直至满足停止条件。这样就生成了一颗CART回归树，假设最终将特征空间被划分为了$M$个子空间$R_1,R_2,...,R_M$，那么CART回归树的模型公式可以表示为
  $$
  f(\boldsymbol {x}) = \sum_{m=1}^{M}c_m\mathbb{I}(x\in{R_m})
  $$
  同理，其中的$c_m$表示的也是集合$R_m$中的样本$\boldsymbol {x}_i$对应的输出值$y_i$的均值。此公式直观上的理解就是，对于一个给定的样本$\boldsymbol {x}_i$，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出值作为该样本的预测值$y_i$。

## 参考文献

[1]李航编著.统计学习方法[M].清华大学出版社,2012.
